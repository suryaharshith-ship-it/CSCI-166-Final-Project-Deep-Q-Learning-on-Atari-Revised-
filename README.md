# ğŸ® Deep Q-Learning on Atari (Pong)

This is my final project for the DQN Atari assignment.
I used Pong as my environment and built two agents:

1. **Baseline DQN** (from the starter notebook)
2. **Double DQN (DDQN)** as my required variant

My goal was to compare how the baseline learns vs. how the DDQN version works, and to show actual learning progress with curves and videos.

---

## ğŸ“Œ Project Overview

* **Game:** ALE/Pong-v5
* **Frameworks:** PyTorch, Gymnasium, Stable Baselines 3 wrappers
* **Baseline:** Regular DQN with replay buffer + target network
* **Variant:** Double DQN (online argmax + target net evaluation)
* **Videos:** One early/random and one trained/DDQN

I followed the starter code and added the DDQN update myself.

---

## ğŸ“ Repo Layout

```
/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ baseline_dqn_pong.ipynb      # Baseline DQN run (starter notebook)
â”‚   â””â”€â”€ ddqn_pong_variant.ipynb      # My DDQN version
â”‚
â”œâ”€â”€ videos/
â”‚   â”œâ”€â”€ pong_early_random.mp4        # Random early behavior
â”‚   â””â”€â”€ pong_ddqn_learned.mp4        # Trained DDQN behavior
â”‚
â”œâ”€â”€ plots/
â”‚   â””â”€â”€ training_curve.png           # Baseline vs DDQN curve
â”‚
â””â”€â”€ README.md
```

---

## ğŸ¥ Videos

Here are the two required videos showing the agentâ€™s behavior:

| Video          | Link                            |
| -------------- | ------------------------------- |
| Early / Random | *(add link to the MP4 in repo)* |
| DDQN Learned   | *(add link to the MP4 in repo)* |

---

## ğŸ“ˆ Learning Curves

I plotted both:

* Baseline DQN (using the provided learning curve for now)
* My DDQN training run

The DDQN learns faster and reaches better reward compared to the baseline.

See the plot here:

```
plots/training_curve.png
```

---

## ğŸ”§ Hyperparameters (important ones)

* **Discount (Î³):** 0.99
* **Optimizer:** Adam, LR = 1e-4
* **Replay buffer:** 10,000
* **Batch size:** 32
* **Frame stack:** 4
* **Target sync:** 500 (for my DDQN run)
* **Epsilon:** 1.0 â†’ 0.01 linear decay

---

## ğŸ§ª Methods

* Baseline DQN: Ran the starter notebook as-is.
* DDQN: Modified the loss so the online net picks the best action and the target net evaluates it.
* Logged reward curves and compared performance.
* Recorded videos to show behavior before vs. after training.

---

## ğŸ“ Reflection (Short Version)

Pong is simple but still tricky because rewards are sparse and the agent needs time to figure things out.
The baseline improved slowly, but the DDQN version clearly did better and had more stable updates.

The hardest parts were long training times and tuning epsilon decay.
If I had more time, Iâ€™d try things like prioritized replay, different target sync rates, or maybe N-step returns.

---

## ğŸ”— Starter Notebook Used

[`c166f25_02b_dqn_pong.ipynb`](ADD_LINK_HERE)
